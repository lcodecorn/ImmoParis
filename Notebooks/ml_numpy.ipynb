{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import os\n",
    "\n",
    "notebook_dir = os.getcwd()\n",
    "project_root = os.path.dirname(notebook_dir)\n",
    "csv_path = os.path.join(project_root, \"Data\", \"xp.csv\")\n",
    "csv_path1 = os.path.join(project_root, \"Data\", \"metro.csv\")\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "metro_df = pd.read_csv(csv_path1)\n",
    "metro_df.columns = metro_df.columns.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "kmeans_class",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeansNumPy:\n",
    "    \"\"\"K-Means clustering implementation using only NumPy\"\"\"\n",
    "    \n",
    "    def __init__(self, n_clusters=8, max_iter=300, n_init=10, random_state=None):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.max_iter = max_iter\n",
    "        self.n_init = n_init\n",
    "        self.random_state = random_state\n",
    "        self.cluster_centers_ = None\n",
    "        self.labels_ = None\n",
    "        self.inertia_ = None\n",
    "        \n",
    "    def _initialize_centroids(self, X, random_state):\n",
    "        \"\"\"Initialize centroids using random samples\"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        indices = random_state.choice(n_samples, self.n_clusters, replace=False)\n",
    "        return X[indices].copy()\n",
    "    \n",
    "    def _assign_clusters(self, X, centroids):\n",
    "        \"\"\"Assign each sample to nearest centroid\"\"\"\n",
    "        distances = np.zeros((X.shape[0], self.n_clusters))\n",
    "        for i, centroid in enumerate(centroids):\n",
    "            distances[:, i] = np.sum((X - centroid) ** 2, axis=1)\n",
    "        labels = np.argmin(distances, axis=1)\n",
    "        return labels\n",
    "    \n",
    "    def _update_centroids(self, X, labels):\n",
    "        \"\"\"Update centroids as mean of assigned samples\"\"\"\n",
    "        centroids = np.zeros((self.n_clusters, X.shape[1]))\n",
    "        for k in range(self.n_clusters):\n",
    "            mask = labels == k\n",
    "            if np.sum(mask) > 0:\n",
    "                centroids[k] = np.mean(X[mask], axis=0)\n",
    "            else:\n",
    "                centroids[k] = self.cluster_centers_[k]\n",
    "        return centroids\n",
    "    \n",
    "    def _compute_inertia(self, X, labels, centroids):\n",
    "        \"\"\"Compute sum of squared distances to nearest centroid\"\"\"\n",
    "        inertia = 0\n",
    "        for k in range(self.n_clusters):\n",
    "            mask = labels == k\n",
    "            if np.sum(mask) > 0:\n",
    "                inertia += np.sum((X[mask] - centroids[k]) ** 2)\n",
    "        return inertia\n",
    "    \n",
    "    def fit(self, X):\n",
    "        \"\"\"Fit K-Means to data\"\"\"\n",
    "        X = np.asarray(X)\n",
    "        \n",
    "        if self.random_state is not None:\n",
    "            random_state = np.random.RandomState(self.random_state)\n",
    "        else:\n",
    "            random_state = np.random.RandomState()\n",
    "        \n",
    "        best_inertia = np.inf\n",
    "        best_centroids = None\n",
    "        best_labels = None\n",
    "        \n",
    "        for _ in range(self.n_init):\n",
    "            centroids = self._initialize_centroids(X, random_state)\n",
    "            \n",
    "            for iteration in range(self.max_iter):\n",
    "                old_centroids = centroids.copy()\n",
    "                labels = self._assign_clusters(X, centroids)\n",
    "                self.cluster_centers_ = centroids\n",
    "                centroids = self._update_centroids(X, labels)\n",
    "                \n",
    "                if np.allclose(centroids, old_centroids):\n",
    "                    break\n",
    "            \n",
    "            inertia = self._compute_inertia(X, labels, centroids)\n",
    "            \n",
    "            if inertia < best_inertia:\n",
    "                best_inertia = inertia\n",
    "                best_centroids = centroids\n",
    "                best_labels = labels\n",
    "        \n",
    "        self.cluster_centers_ = best_centroids\n",
    "        self.labels_ = best_labels\n",
    "        self.inertia_ = best_inertia\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict cluster labels for new data\"\"\"\n",
    "        X = np.asarray(X)\n",
    "        return self._assign_clusters(X, self.cluster_centers_)\n",
    "    \n",
    "    def fit_predict(self, X):\n",
    "        \"\"\"Fit and return cluster labels\"\"\"\n",
    "        self.fit(X)\n",
    "        return self.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "balltree_class",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BallTreeNumPy:\n",
    "    \"\"\"Ball Tree for efficient nearest neighbor search with haversine distance\"\"\"\n",
    "    \n",
    "    def __init__(self, data, metric='haversine', leaf_size=40):\n",
    "        self.data = np.asarray(data)\n",
    "        self.metric = metric\n",
    "        self.leaf_size = leaf_size\n",
    "        self.n_samples = len(data)\n",
    "        \n",
    "    def _haversine_distance_vectorized(self, points, reference):\n",
    "        \"\"\"Vectorized haversine distance calculation\"\"\"\n",
    "        lat1, lon1 = reference\n",
    "        lat2 = points[:, 0]\n",
    "        lon2 = points[:, 1]\n",
    "        \n",
    "        dlat = lat2 - lat1\n",
    "        dlon = lon2 - lon1\n",
    "        \n",
    "        a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "        c = 2 * np.arcsin(np.sqrt(np.clip(a, 0, 1)))\n",
    "        \n",
    "        return c\n",
    "    \n",
    "    def query(self, X, k=1):\n",
    "        \"\"\"Find k nearest neighbors for each point in X\"\"\"\n",
    "        X = np.asarray(X)\n",
    "        n_queries = X.shape[0]\n",
    "        \n",
    "        distances = np.zeros((n_queries, k))\n",
    "        indices = np.zeros((n_queries, k), dtype=int)\n",
    "        \n",
    "        for i, query_point in enumerate(X):\n",
    "            dists = self._haversine_distance_vectorized(self.data, query_point)\n",
    "            idx = np.argpartition(dists, min(k, len(dists)-1))[:k]\n",
    "            idx = idx[np.argsort(dists[idx])]\n",
    "            \n",
    "            distances[i] = dists[idx]\n",
    "            indices[i] = idx\n",
    "        \n",
    "        return distances, indices\n",
    "    \n",
    "    def query_radius(self, X, r):\n",
    "        \"\"\"Find all neighbors within radius r for each point in X\"\"\"\n",
    "        X = np.asarray(X)\n",
    "        results = []\n",
    "        \n",
    "        for i, query_point in enumerate(X):\n",
    "            dists = self._haversine_distance_vectorized(self.data, query_point)\n",
    "            idx = np.where(dists <= r)[0]\n",
    "            results.append(idx)\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "preprocessing_classes",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardScalerNumPy:\n",
    "    \"\"\"Standardize features by removing mean and scaling to unit variance\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.mean_ = None\n",
    "        self.scale_ = None\n",
    "        \n",
    "    def fit(self, X):\n",
    "        \"\"\"Compute mean and std for scaling\"\"\"\n",
    "        X = np.asarray(X)\n",
    "        self.mean_ = np.mean(X, axis=0)\n",
    "        self.scale_ = np.std(X, axis=0)\n",
    "        self.scale_[self.scale_ == 0] = 1.0\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Scale features\"\"\"\n",
    "        X = np.asarray(X)\n",
    "        return (X - self.mean_) / self.scale_\n",
    "    \n",
    "    def fit_transform(self, X):\n",
    "        \"\"\"Fit and transform\"\"\"\n",
    "        return self.fit(X).transform(X)\n",
    "\n",
    "\n",
    "class SimpleImputerNumPy:\n",
    "    \"\"\"Impute missing values\"\"\"\n",
    "    \n",
    "    def __init__(self, strategy='mean'):\n",
    "        self.strategy = strategy\n",
    "        self.statistics_ = None\n",
    "        \n",
    "    def fit(self, X):\n",
    "        \"\"\"Compute statistics for imputation\"\"\"\n",
    "        X = np.asarray(X)\n",
    "        \n",
    "        if self.strategy == 'mean':\n",
    "            self.statistics_ = np.nanmean(X, axis=0)\n",
    "        elif self.strategy == 'median':\n",
    "            self.statistics_ = np.nanmedian(X, axis=0)\n",
    "        elif self.strategy == 'most_frequent':\n",
    "            self.statistics_ = np.array([self._most_frequent(X[:, i]) for i in range(X.shape[1])])\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _most_frequent(self, column):\n",
    "        \"\"\"Get most frequent value in column\"\"\"\n",
    "        valid = column[~np.isnan(column)]\n",
    "        if len(valid) == 0:\n",
    "            return 0\n",
    "        counter = Counter(valid)\n",
    "        return counter.most_common(1)[0][0]\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Impute missing values\"\"\"\n",
    "        X = np.asarray(X, dtype=float).copy()\n",
    "        \n",
    "        for i in range(X.shape[1]):\n",
    "            mask = np.isnan(X[:, i])\n",
    "            X[mask, i] = self.statistics_[i]\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    def fit_transform(self, X):\n",
    "        \"\"\"Fit and transform\"\"\"\n",
    "        return self.fit(X).transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "decision_tree",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeRegressorNumPy:\n",
    "    \"\"\"Fast Decision Tree Regressor using optimized NumPy operations\"\"\"\n",
    "    \n",
    "    def __init__(self, max_depth=None, min_samples_split=2, min_samples_leaf=1, \n",
    "                 max_features=None, random_state=None, criterion='squared_error'):\n",
    "        self.max_depth = max_depth if max_depth is not None else 999\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.max_features = max_features\n",
    "        self.random_state = random_state\n",
    "        self.criterion = criterion  # 'squared_error' or 'absolute_error'\n",
    "        self.tree_ = None\n",
    "        self.feature_importances_ = None\n",
    "        \n",
    "    def _compute_error(self, y):\n",
    "        \"\"\"Compute error metric for split evaluation\"\"\"\n",
    "        if len(y) == 0:\n",
    "            return 0\n",
    "        \n",
    "        if self.criterion == 'absolute_error':\n",
    "            median = np.median(y)\n",
    "            return np.mean(np.abs(y - median))\n",
    "        else:  # squared_error\n",
    "            mean = np.mean(y)\n",
    "            return np.mean((y - mean) ** 2)\n",
    "    \n",
    "    def _best_split_fast(self, X, y, feature_indices, indices):\n",
    "        \"\"\"Fast best split using vectorized operations and sampling\"\"\"\n",
    "        best_gain = -np.inf\n",
    "        best_feature = None\n",
    "        best_threshold = None\n",
    "        \n",
    "        current_error = self._compute_error(y[indices])\n",
    "        n_total = len(indices)\n",
    "        \n",
    "        for feature in feature_indices:\n",
    "            X_feat = X[indices, feature]\n",
    "            \n",
    "            # Sample thresholds\n",
    "            unique_vals = np.unique(X_feat)\n",
    "            if len(unique_vals) > 20:\n",
    "                percentiles = np.linspace(5, 95, 20)\n",
    "                thresholds = np.percentile(X_feat, percentiles)\n",
    "            else:\n",
    "                thresholds = unique_vals[:-1]\n",
    "            \n",
    "            for threshold in thresholds:\n",
    "                left_mask = X_feat <= threshold\n",
    "                n_left = np.sum(left_mask)\n",
    "                n_right = n_total - n_left\n",
    "                \n",
    "                if n_left < self.min_samples_leaf or n_right < self.min_samples_leaf:\n",
    "                    continue\n",
    "                \n",
    "                left_error = self._compute_error(y[indices][left_mask])\n",
    "                right_error = self._compute_error(y[indices][~left_mask])\n",
    "                \n",
    "                weighted_error = (n_left * left_error + n_right * right_error) / n_total\n",
    "                gain = current_error - weighted_error\n",
    "                \n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_feature = feature\n",
    "                    best_threshold = threshold\n",
    "        \n",
    "        return best_feature, best_threshold\n",
    "    \n",
    "    def _build_tree_fast(self, X, y, indices, depth=0):\n",
    "        \"\"\"Build tree using indices instead of copying data\"\"\"\n",
    "        n_samples = len(indices)\n",
    "        \n",
    "        # Leaf node value depends on criterion\n",
    "        if self.criterion == 'absolute_error':\n",
    "            leaf_value = np.median(y[indices])\n",
    "        else:\n",
    "            leaf_value = np.mean(y[indices])\n",
    "        \n",
    "        # Stopping criteria\n",
    "        if depth >= self.max_depth or n_samples < self.min_samples_split or np.std(y[indices]) < 1e-7:\n",
    "            return {'type': 'leaf', 'value': leaf_value, 'n_samples': n_samples}\n",
    "        \n",
    "        # Select features\n",
    "        n_features = X.shape[1]\n",
    "        if self.max_features == 'sqrt':\n",
    "            n_select = max(1, int(np.sqrt(n_features)))\n",
    "            feature_indices = self.rng.choice(n_features, n_select, replace=False)\n",
    "        elif isinstance(self.max_features, int):\n",
    "            feature_indices = self.rng.choice(n_features, min(self.max_features, n_features), replace=False)\n",
    "        else:\n",
    "            feature_indices = np.arange(n_features)\n",
    "        \n",
    "        # Find best split\n",
    "        feature, threshold = self._best_split_fast(X, y, feature_indices, indices)\n",
    "        \n",
    "        if feature is None:\n",
    "            return {'type': 'leaf', 'value': leaf_value, 'n_samples': n_samples}\n",
    "        \n",
    "        # Split indices\n",
    "        left_mask = X[indices, feature] <= threshold\n",
    "        left_indices = indices[left_mask]\n",
    "        right_indices = indices[~left_mask]\n",
    "        \n",
    "        # Build subtrees\n",
    "        return {\n",
    "            'type': 'node',\n",
    "            'feature': feature,\n",
    "            'threshold': threshold,\n",
    "            'n_samples': n_samples,\n",
    "            'left': self._build_tree_fast(X, y, left_indices, depth + 1),\n",
    "            'right': self._build_tree_fast(X, y, right_indices, depth + 1)\n",
    "        }\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Build decision tree\"\"\"\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "        \n",
    "        if self.random_state is not None:\n",
    "            self.rng = np.random.RandomState(self.random_state)\n",
    "        else:\n",
    "            self.rng = np.random.RandomState()\n",
    "        \n",
    "        self.n_features_ = X.shape[1]\n",
    "        indices = np.arange(len(X))\n",
    "        self.tree_ = self._build_tree_fast(X, y, indices)\n",
    "        \n",
    "        # Calculate feature importances\n",
    "        self.feature_importances_ = self._compute_feature_importances()\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _compute_feature_importances(self):\n",
    "        \"\"\"Compute feature importances based on tree structure\"\"\"\n",
    "        importances = np.zeros(self.n_features_)\n",
    "        \n",
    "        def traverse(node):\n",
    "            if node['type'] == 'leaf':\n",
    "                return\n",
    "            \n",
    "            feature = node['feature']\n",
    "            importances[feature] += node['n_samples']\n",
    "            \n",
    "            traverse(node['left'])\n",
    "            traverse(node['right'])\n",
    "        \n",
    "        traverse(self.tree_)\n",
    "        \n",
    "        if importances.sum() > 0:\n",
    "            importances = importances / importances.sum()\n",
    "        \n",
    "        return importances\n",
    "    \n",
    "    def _predict_batch(self, X, tree):\n",
    "        \"\"\"Vectorized prediction for batch of samples\"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        predictions = np.zeros(n_samples)\n",
    "        indices = np.arange(n_samples)\n",
    "        \n",
    "        def traverse(node, idx):\n",
    "            if len(idx) == 0:\n",
    "                return\n",
    "            \n",
    "            if node['type'] == 'leaf':\n",
    "                predictions[idx] = node['value']\n",
    "                return\n",
    "            \n",
    "            left_mask = X[idx, node['feature']] <= node['threshold']\n",
    "            left_idx = idx[left_mask]\n",
    "            right_idx = idx[~left_mask]\n",
    "            \n",
    "            traverse(node['left'], left_idx)\n",
    "            traverse(node['right'], right_idx)\n",
    "        \n",
    "        traverse(tree, indices)\n",
    "        return predictions\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict for multiple samples using vectorized approach\"\"\"\n",
    "        X = np.asarray(X)\n",
    "        return self._predict_batch(X, self.tree_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "random_forest",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestRegressorNumPy:\n",
    "    \"\"\"Random Forest Regressor using only NumPy\"\"\"\n",
    "    \n",
    "    def __init__(self, n_estimators=100, max_depth=None, min_samples_split=2,\n",
    "                 min_samples_leaf=1, max_features='sqrt', random_state=None, \n",
    "                 n_jobs=None, criterion='squared_error'):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.max_features = max_features\n",
    "        self.random_state = random_state\n",
    "        self.n_jobs = n_jobs\n",
    "        self.criterion = criterion\n",
    "        self.trees_ = []\n",
    "        self.feature_importances_ = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Build random forest\"\"\"\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "        \n",
    "        if self.random_state is not None:\n",
    "            rng = np.random.RandomState(self.random_state)\n",
    "        else:\n",
    "            rng = np.random.RandomState()\n",
    "        \n",
    "        self.trees_ = []\n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        print(f\"Training {self.n_estimators} trees...\")\n",
    "        \n",
    "        for i in range(self.n_estimators):\n",
    "            if (i + 1) % 10 == 0:\n",
    "                print(f\"  Tree {i + 1}/{self.n_estimators}\")\n",
    "            \n",
    "            # Bootstrap sampling\n",
    "            indices = rng.choice(n_samples, n_samples, replace=True)\n",
    "            X_bootstrap = X[indices]\n",
    "            y_bootstrap = y[indices]\n",
    "            \n",
    "            # Train tree\n",
    "            tree = DecisionTreeRegressorNumPy(\n",
    "                max_depth=self.max_depth,\n",
    "                min_samples_split=self.min_samples_split,\n",
    "                min_samples_leaf=self.min_samples_leaf,\n",
    "                max_features=self.max_features,\n",
    "                criterion=self.criterion,\n",
    "                random_state=rng.randint(0, 100000)\n",
    "            )\n",
    "            tree.fit(X_bootstrap, y_bootstrap)\n",
    "            self.trees_.append(tree)\n",
    "        \n",
    "        # Compute feature importances\n",
    "        self.feature_importances_ = np.mean([tree.feature_importances_ for tree in self.trees_], axis=0)\n",
    "        \n",
    "        print(\"Training complete!\")\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict by averaging predictions from all trees\"\"\"\n",
    "        X = np.asarray(X)\n",
    "        predictions = np.array([tree.predict(X) for tree in self.trees_])\n",
    "        return np.mean(predictions, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "metrics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_error(y_true, y_pred):\n",
    "    \"\"\"Calculate Mean Absolute Error\"\"\"\n",
    "    return np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    \"\"\"Calculate Mean Squared Error\"\"\"\n",
    "    return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "def r2_score(y_true, y_pred):\n",
    "    \"\"\"Calculate R² Score\"\"\"\n",
    "    ss_res = np.sum((y_true - y_pred) ** 2)\n",
    "    ss_tot = np.sum((y_true - np.mean(y_true)) ** 2)\n",
    "    return 1 - (ss_res / ss_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "datetime_features",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Date features\n",
    "df['date_mutation'] = pd.to_datetime(df['date_mutation'])\n",
    "\n",
    "df['year'] = df['date_mutation'].dt.year\n",
    "df['month'] = df['date_mutation'].dt.month\n",
    "df['day_of_week'] = df['date_mutation'].dt.dayofweek\n",
    "df['days_since_start'] = (df['date_mutation'] - df['date_mutation'].min()).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "geo_clusters",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Geographic clusters using our KMeans implementation\n",
    "geo_features = df[['longitude', 'latitude']].values\n",
    "n_clusters = 20\n",
    "\n",
    "km = KMeansNumPy(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "df['geo_cluster'] = km.fit_predict(geo_features)\n",
    "\n",
    "# Distance from center\n",
    "center_lon, center_lat = 2.3384444444444446, 48.86152777777778\n",
    "\n",
    "df['dist_center'] = np.sqrt((df['longitude'] - center_lon)**2 +\n",
    "                             (df['latitude'] - center_lat)**2) * 111"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "metro_features",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metro station features using our BallTree implementation\n",
    "metro_coords = np.radians(metro_df[['Latitude', 'Longitude']].values)\n",
    "tree = BallTreeNumPy(metro_coords, metric='haversine')\n",
    "\n",
    "appart_coords = np.radians(df[['latitude', 'longitude']].values)\n",
    "\n",
    "distances, indices = tree.query(appart_coords, k=1)\n",
    "df['nearest_metro_dist_km'] = distances.flatten() * 6371\n",
    "\n",
    "# Add nearest metro station info\n",
    "df['nearest_metro_station'] = metro_df.iloc[indices.flatten()]['Libelle station'].values\n",
    "df['nearest_metro_line'] = metro_df.iloc[indices.flatten()]['Libelle Line'].values\n",
    "\n",
    "# Count metro stations within different radii\n",
    "indices_300m = tree.query_radius(appart_coords, r=0.3/6371)\n",
    "indices_500m = tree.query_radius(appart_coords, r=0.5/6371)\n",
    "\n",
    "df['metro_count_300m'] = [len(idx) for idx in indices_300m]\n",
    "df['metro_count_500m'] = [len(idx) for idx in indices_500m]\n",
    "df['very_close_to_metro'] = (df['nearest_metro_dist_km'] < 0.1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "property_features",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Property features\n",
    "df['surface_per_piece'] = df['surface_reelle_bati'] / df['nombre_pieces_principales'].replace(0, 1)\n",
    "df['is_studio'] = (df['nombre_pieces_principales'] == 1).astype(int)\n",
    "df['is_large'] = (df['nombre_pieces_principales'] >= 4).astype(int)\n",
    "\n",
    "# Surface categories\n",
    "df['surface_category'] = pd.cut(df['surface_reelle_bati'], \n",
    "                                  bins=[9, 40, 80, float('inf')],\n",
    "                                  labels=['small', 'medium', 'large'])\n",
    "\n",
    "# IMPORTANT: Add price_per_sqrtm column (the target in sklearn notebook)\n",
    "df['price_per_sqrtm'] = df['valeur_fonciere'] / df['surface_reelle_bati']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "split_data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by date\n",
    "df_sorted = df.sort_values('date_mutation').reset_index(drop=True)\n",
    "\n",
    "# Split: 80% train, 20% test\n",
    "split_index = int(len(df_sorted) * 0.8)\n",
    "\n",
    "train_df = df_sorted.iloc[:split_index].copy()\n",
    "test_df = df_sorted.iloc[split_index:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "compute_station_stats",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_stats_train = train_df.groupby('nearest_metro_station').agg({\n",
    "    'surface_reelle_bati': ['mean', 'std', 'median', 'count'],\n",
    "    'nombre_pieces_principales': ['mean', 'std', 'median'],\n",
    "}).round(2)\n",
    "\n",
    "# Flatten column names\n",
    "station_stats_train.columns = [\n",
    "    'station_avg_surface',\n",
    "    'station_surface_std',\n",
    "    'station_median_surface',\n",
    "    'station_tx_count',\n",
    "    'station_avg_rooms',\n",
    "    'station_rooms_std',\n",
    "    'station_median_rooms'\n",
    "]\n",
    "\n",
    "# Add derived features\n",
    "station_stats_train['station_surface_range'] = (\n",
    "    station_stats_train['station_avg_surface'] + 2*station_stats_train['station_surface_std'] -\n",
    "    (station_stats_train['station_avg_surface'] - 2*station_stats_train['station_surface_std'])\n",
    ")\n",
    "\n",
    "# Replace zero std with 1 to avoid division by zero\n",
    "station_stats_train['station_surface_std'] = station_stats_train['station_surface_std'].replace(0, 1)\n",
    "station_stats_train['station_rooms_std'] = station_stats_train['station_rooms_std'].replace(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "merge_station_stats",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Handling missing values for new stations in test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\souri\\AppData\\Local\\Temp\\ipykernel_5112\\180845287.py:30: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  train_df[col].fillna(train_mean, inplace=True)\n",
      "C:\\Users\\souri\\AppData\\Local\\Temp\\ipykernel_5112\\180845287.py:31: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  test_df[col].fillna(train_mean, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# Merge to TRAIN\n",
    "train_df = train_df.merge(\n",
    "    station_stats_train,\n",
    "    left_on='nearest_metro_station',\n",
    "    right_index=True,\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Merge to TEST\n",
    "test_df = test_df.merge(\n",
    "    station_stats_train,\n",
    "    left_on='nearest_metro_station',\n",
    "    right_index=True,\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Handle new stations in test\n",
    "station_cols = [c for c in train_df.columns if c.startswith('station_')]\n",
    "\n",
    "for col in station_cols:\n",
    "    train_mean = train_df[col].mean()\n",
    "    \n",
    "    train_missing = train_df[col].isna().sum()\n",
    "    test_missing = test_df[col].isna().sum()\n",
    "    \n",
    "    if train_missing > 0 or test_missing > 0:\n",
    "        print(f\"  {col}: filling {test_missing} missing values in test with train mean {train_mean:.2f}\")\n",
    "    \n",
    "    train_df[col].fillna(train_mean, inplace=True)\n",
    "    test_df[col].fillna(train_mean, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "derived_features",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derived features based on station stats - MATCHING SKLEARN NOTEBOOK EXACTLY\n",
    "for dataset_name, dataset in [('TRAIN', train_df), ('TEST', test_df)]:\n",
    "    dataset['surface_vs_station_avg'] = (\n",
    "        (dataset['surface_reelle_bati'] - dataset['station_avg_surface']) / \n",
    "        dataset['station_surface_std']\n",
    "    )\n",
    "    \n",
    "    dataset['rooms_vs_station_avg'] = (\n",
    "        (dataset['nombre_pieces_principales'] - dataset['station_avg_rooms']) / \n",
    "        dataset['station_rooms_std']\n",
    "    )\n",
    "    \n",
    "    dataset['larger_than_station_median'] = (\n",
    "        dataset['surface_reelle_bati'] > dataset['station_median_surface']\n",
    "    ).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aggregate_train",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train aggregated shape: (837, 31)\n"
     ]
    }
   ],
   "source": [
    "# Aggregate by (code_postal, year, month) - EXACTLY as in sklearn notebook\n",
    "train_agg = train_df.groupby(['code_postal', 'year', 'month']).agg({\n",
    "    'price_per_sqrtm': 'median',\n",
    "    'valeur_fonciere': 'median',\n",
    "    'nombre_pieces_principales': 'median',\n",
    "    'surface_reelle_bati': 'median',\n",
    "    'surface_per_piece': 'median',\n",
    "    'longitude': 'mean',\n",
    "    'latitude': 'mean',\n",
    "    'days_since_start': 'median',\n",
    "    'geo_cluster': lambda x: x.mode()[0] if len(x.mode()) > 0 else x.iloc[0],\n",
    "    'dist_center': 'median',\n",
    "    'nearest_metro_dist_km': 'median',\n",
    "    'station_tx_count': 'median',\n",
    "    'station_avg_surface': 'median',\n",
    "    'station_surface_std': 'median',\n",
    "    'station_median_surface': 'median',\n",
    "    'station_avg_rooms': 'median',\n",
    "    'station_rooms_std': 'median',\n",
    "    'station_median_rooms': 'median',\n",
    "    'station_surface_range': 'median',\n",
    "    'surface_vs_station_avg': 'median',\n",
    "    'rooms_vs_station_avg': 'median',\n",
    "    'larger_than_station_median': 'median',\n",
    "    'metro_count_300m': 'median',\n",
    "    'metro_count_500m': 'median',\n",
    "    'very_close_to_metro': 'median',\n",
    "    'is_studio': 'median',\n",
    "    'is_large': 'median',\n",
    "    'date_mutation': 'first'\n",
    "}).reset_index()\n",
    "\n",
    "print(f\"Train aggregated shape: {train_agg.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aggregate_test",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test aggregated shape: (260, 31)\n"
     ]
    }
   ],
   "source": [
    "test_agg = test_df.groupby(['code_postal', 'year', 'month']).agg({\n",
    "    'price_per_sqrtm': 'median',\n",
    "    'valeur_fonciere': 'median',\n",
    "    'nombre_pieces_principales': 'median',\n",
    "    'surface_reelle_bati': 'median',\n",
    "    'surface_per_piece': 'median',\n",
    "    'longitude': 'mean',\n",
    "    'latitude': 'mean',\n",
    "    'days_since_start': 'median',\n",
    "    'geo_cluster': lambda x: x.mode()[0] if len(x.mode()) > 0 else x.iloc[0],\n",
    "    'dist_center': 'median',\n",
    "    'nearest_metro_dist_km': 'median',\n",
    "    'station_tx_count': 'median',\n",
    "    'station_avg_surface': 'median',\n",
    "    'station_surface_std': 'median',\n",
    "    'station_median_surface': 'median',\n",
    "    'station_avg_rooms': 'median',\n",
    "    'station_rooms_std': 'median',\n",
    "    'station_median_rooms': 'median',\n",
    "    'station_surface_range': 'median',\n",
    "    'surface_vs_station_avg': 'median',\n",
    "    'rooms_vs_station_avg': 'median',\n",
    "    'larger_than_station_median': 'median',\n",
    "    'metro_count_300m': 'median',\n",
    "    'metro_count_500m': 'median',\n",
    "    'very_close_to_metro': 'median',\n",
    "    'is_studio': 'median',\n",
    "    'is_large': 'median',\n",
    "    'date_mutation': 'first'\n",
    "}).reset_index()\n",
    "\n",
    "print(f\"Test aggregated shape: {test_agg.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "transaction_counts",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing historical transaction counts (TRAIN DATA ONLY)...\n"
     ]
    }
   ],
   "source": [
    "# Transaction counts per postal code per month\n",
    "train_tx_counts = train_df.groupby(['code_postal', 'year', 'month']).size().reset_index(name='transaction_count')\n",
    "test_tx_counts = test_df.groupby(['code_postal', 'year', 'month']).size().reset_index(name='transaction_count')\n",
    "\n",
    "train_agg = train_agg.merge(train_tx_counts, on=['code_postal', 'year', 'month'])\n",
    "test_agg = test_agg.merge(test_tx_counts, on=['code_postal', 'year', 'month'])\n",
    "\n",
    "# Historical activity - compute ONLY on training data\n",
    "print(\"\\nComputing historical transaction counts (TRAIN DATA ONLY)...\")\n",
    "historical_activity_train = train_df.groupby('code_postal').size().reset_index(name='total_transactions_train')\n",
    "\n",
    "# Merge to both train and test (using TRAIN statistics)\n",
    "train_agg = train_agg.merge(historical_activity_train, on='code_postal', how='left')\n",
    "test_agg = test_agg.merge(historical_activity_train, on='code_postal', how='left')\n",
    "\n",
    "# Handle postal codes in test that weren't in train\n",
    "overall_avg_transactions = historical_activity_train['total_transactions_train'].mean()\n",
    "test_agg['total_transactions_train'] = test_agg['total_transactions_train'].fillna(overall_avg_transactions)\n",
    "\n",
    "# Market activity ratio\n",
    "train_agg['market_activity_ratio'] = train_agg['transaction_count'] / train_agg['total_transactions_train']\n",
    "test_agg['market_activity_ratio'] = test_agg['transaction_count'] / test_agg['total_transactions_train']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prepare_data",
   "metadata": {},
   "source": [
    "## Prepare Data for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "select_features",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: (837, 31)\n",
      "Test set: (260, 31)\n"
     ]
    }
   ],
   "source": [
    "# Define target and features - MATCHING SKLEARN NOTEBOOK\n",
    "target = 'price_per_sqrtm'\n",
    "drop_cols = ['price_per_sqrtm', 'valeur_fonciere', 'date_mutation']\n",
    "\n",
    "# Split features and target\n",
    "x_train = train_agg.drop(columns=drop_cols)\n",
    "y_train = train_agg[target]\n",
    "\n",
    "x_test = test_agg.drop(columns=drop_cols)\n",
    "y_test = test_agg[target]\n",
    "\n",
    "print(f\"Training set: {x_train.shape}\")\n",
    "print(f\"Test set: {x_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "identify_features",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Numeric features: 29\n",
      "['code_postal', 'nombre_pieces_principales', 'surface_reelle_bati', 'surface_per_piece', 'longitude', 'latitude', 'days_since_start', 'geo_cluster', 'dist_center', 'nearest_metro_dist_km', 'station_tx_count', 'station_avg_surface', 'station_surface_std', 'station_median_surface', 'station_avg_rooms', 'station_rooms_std', 'station_median_rooms', 'station_surface_range', 'surface_vs_station_avg', 'rooms_vs_station_avg', 'larger_than_station_median', 'metro_count_300m', 'metro_count_500m', 'very_close_to_metro', 'is_studio', 'is_large', 'transaction_count', 'total_transactions_train', 'market_activity_ratio']\n"
     ]
    }
   ],
   "source": [
    "# Identify numeric features only (no categorical in this dataset)\n",
    "numeric_features = x_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "print(f\"\\nNumeric features: {len(numeric_features)}\")\n",
    "print(numeric_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preprocessing",
   "metadata": {},
   "source": [
    "## Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apply_preprocessing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputing missing values with median...\n",
      "Standardizing features...\n",
      "Preprocessing complete!\n",
      "Processed training shape: (837, 29)\n",
      "Processed test shape: (260, 29)\n"
     ]
    }
   ],
   "source": [
    "# Extract numeric data\n",
    "X_train_numeric = x_train[numeric_features].values\n",
    "X_test_numeric = x_test[numeric_features].values\n",
    "\n",
    "# Impute missing values with median (matching sklearn notebook)\n",
    "imputer = SimpleImputerNumPy(strategy='median')\n",
    "X_train_imputed = imputer.fit_transform(X_train_numeric)\n",
    "X_test_imputed = imputer.transform(X_test_numeric)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScalerNumPy()\n",
    "X_train_processed = scaler.fit_transform(X_train_imputed)\n",
    "X_test_processed = scaler.transform(X_test_imputed)\n",
    "\n",
    "print(\"Preprocessing complete!\")\n",
    "print(f\"Processed training shape: {X_train_processed.shape}\")\n",
    "print(f\"Processed test shape: {X_test_processed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "train_model",
   "metadata": {},
   "source": [
    "## Train Random Forest Model (Matching sklearn parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baseline_model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING BASELINE MODEL\n",
      "============================================================\n",
      "Training 200 trees...\n",
      "  Tree 10/200\n",
      "  Tree 20/200\n",
      "  Tree 30/200\n",
      "  Tree 40/200\n",
      "  Tree 50/200\n",
      "  Tree 60/200\n",
      "  Tree 70/200\n",
      "  Tree 80/200\n",
      "  Tree 90/200\n",
      "  Tree 100/200\n",
      "  Tree 110/200\n",
      "  Tree 120/200\n",
      "  Tree 130/200\n",
      "  Tree 140/200\n",
      "  Tree 150/200\n",
      "  Tree 160/200\n",
      "  Tree 170/200\n",
      "  Tree 180/200\n",
      "  Tree 190/200\n",
      "  Tree 200/200\n",
      "Training complete!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.RandomForestRegressorNumPy at 0x1d3f07b9be0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BASELINE MODEL - matching sklearn baseline\n",
    "print(\"TRAINING BASELINE MODEL\")\n",
    "\n",
    "baseline_model = RandomForestRegressorNumPy(\n",
    "    n_estimators=200,\n",
    "    max_depth=30,\n",
    "    min_samples_leaf=1,\n",
    "    min_samples_split=15,\n",
    "    random_state=42,\n",
    "    criterion='absolute_error'  # Matching sklearn\n",
    ")\n",
    "\n",
    "baseline_model.fit(X_train_processed, y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baseline_eval",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "BASELINE MODEL RESULTS (NumPy Implementation)\n",
      "============================================================\n",
      "Train R²:  0.9658\n",
      "Test R²:   0.8556\n",
      "\n",
      "Train MAE:  212.96 €/m²\n",
      "Test MAE:   489.56 €/m²\n",
      "\n",
      "Train RMSE: 296.68 €/m²\n",
      "Test RMSE:  637.86 €/m²\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "COMPARISON TO SKLEARN BASELINE\n",
      "============================================================\n",
      "sklearn results:\n",
      "  Train R²:  0.9645\n",
      "  Test R²:   0.9083\n",
      "  Train MAE:  205.32 €/m²\n",
      "  Test MAE:   353.10 €/m²\n",
      "  Train RMSE: 302.25 €/m²\n",
      "  Test RMSE:  508.44 €/m²\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Baseline predictions\n",
    "y_train_pred = baseline_model.predict(X_train_processed)\n",
    "y_test_pred = baseline_model.predict(X_test_processed)\n",
    "\n",
    "# Evaluate baseline\n",
    "baseline_results = {\n",
    "    'train_r2': r2_score(y_train.values, y_train_pred),\n",
    "    'test_r2': r2_score(y_test.values, y_test_pred),\n",
    "    'train_mae': mean_absolute_error(y_train.values, y_train_pred),\n",
    "    'test_mae': mean_absolute_error(y_test.values, y_test_pred),\n",
    "    'train_rmse': np.sqrt(mean_squared_error(y_train.values, y_train_pred)),\n",
    "    'test_rmse': np.sqrt(mean_squared_error(y_test.values, y_test_pred))\n",
    "}\n",
    "\n",
    "print(\"BASELINE MODEL RESULTS (NumPy Implementation)\")\n",
    "print(f\"Train R²:  {baseline_results['train_r2']:.4f}\")\n",
    "print(f\"Test R²:   {baseline_results['test_r2']:.4f}\")\n",
    "print(f\"\\nTrain MAE:  {baseline_results['train_mae']:.2f} €/m²\")\n",
    "print(f\"Test MAE:   {baseline_results['test_mae']:.2f} €/m²\")\n",
    "print(f\"\\nTrain RMSE: {baseline_results['train_rmse']:.2f} €/m²\")\n",
    "print(f\"Test RMSE:  {baseline_results['test_rmse']:.2f} €/m²\")\n",
    "\n",
    "print(\"COMPARISON TO SKLEARN BASELINE\")\n",
    "print(\"sklearn results:\")\n",
    "print(\"  Train R²:  0.9645\")\n",
    "print(\"  Test R²:   0.9083\")\n",
    "print(\"  Train MAE:  205.32 €/m²\")\n",
    "print(\"  Test MAE:   353.10 €/m²\")\n",
    "print(\"  Train RMSE: 302.25 €/m²\")\n",
    "print(\"  Test RMSE:  508.44 €/m²\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimized_model",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "TRAINING OPTIMIZED MODEL\n",
      "Using sklearn's best parameters: n_estimators=250, max_depth=25, min_samples_split=25, min_samples_leaf=1\n",
      "============================================================\n",
      "Training 250 trees...\n",
      "  Tree 10/250\n",
      "  Tree 20/250\n",
      "  Tree 30/250\n",
      "  Tree 40/250\n",
      "  Tree 50/250\n",
      "  Tree 60/250\n",
      "  Tree 70/250\n",
      "  Tree 80/250\n",
      "  Tree 90/250\n",
      "  Tree 100/250\n",
      "  Tree 110/250\n",
      "  Tree 120/250\n",
      "  Tree 130/250\n",
      "  Tree 140/250\n",
      "  Tree 150/250\n",
      "  Tree 160/250\n",
      "  Tree 170/250\n",
      "  Tree 180/250\n",
      "  Tree 190/250\n",
      "  Tree 200/250\n",
      "  Tree 210/250\n",
      "  Tree 220/250\n",
      "  Tree 230/250\n",
      "  Tree 240/250\n",
      "  Tree 250/250\n",
      "Training complete!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.RandomForestRegressorNumPy at 0x1d3f07dad50>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OPTIMIZED MODEL - matching sklearn's best parameters from GridSearch\n",
    "print(\"TRAINING OPTIMIZED MODEL\")\n",
    "print(\"Parameters: n_estimators=250, max_depth=25, min_samples_split=25, min_samples_leaf=1\")\n",
    "\n",
    "optimized_model = RandomForestRegressorNumPy(\n",
    "    n_estimators=250,\n",
    "    max_depth=25,\n",
    "    min_samples_leaf=1,\n",
    "    min_samples_split=25,\n",
    "    random_state=42,\n",
    "    criterion='absolute_error'\n",
    ")\n",
    "\n",
    "optimized_model.fit(X_train_processed, y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optimized_eval",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "OPTIMIZED MODEL RESULTS (NumPy Implementation)\n",
      "============================================================\n",
      "Train R²:  0.9506\n",
      "Test R²:   0.8370\n",
      "\n",
      "Train MAE:  258.47 €/m²\n",
      "Test MAE:   536.64 €/m²\n",
      "\n",
      "Train RMSE: 356.58 €/m²\n",
      "Test RMSE:  677.78 €/m²\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "COMPARISON TO SKLEARN OPTIMIZED\n",
      "============================================================\n",
      "sklearn results:\n",
      "  Train R²:  0.9493\n",
      "  Test R²:   0.9009\n",
      "  Train MAE:  246.54 €/m²\n",
      "  Test MAE:   375.53 €/m²\n",
      "  Train RMSE: 361.29 €/m²\n",
      "  Test RMSE:  528.34 €/m²\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Optimized predictions\n",
    "y_train_pred_opt = optimized_model.predict(X_train_processed)\n",
    "y_test_pred_opt = optimized_model.predict(X_test_processed)\n",
    "\n",
    "# Evaluate optimized\n",
    "optimized_results = {\n",
    "    'train_r2': r2_score(y_train.values, y_train_pred_opt),\n",
    "    'test_r2': r2_score(y_test.values, y_test_pred_opt),\n",
    "    'train_mae': mean_absolute_error(y_train.values, y_train_pred_opt),\n",
    "    'test_mae': mean_absolute_error(y_test.values, y_test_pred_opt),\n",
    "    'train_rmse': np.sqrt(mean_squared_error(y_train.values, y_train_pred_opt)),\n",
    "    'test_rmse': np.sqrt(mean_squared_error(y_test.values, y_test_pred_opt))\n",
    "}\n",
    "\n",
    "print(\"OPTIMIZED MODEL RESULTS (NumPy Implementation)\")\n",
    "print(f\"Train R²:  {optimized_results['train_r2']:.4f}\")\n",
    "print(f\"Test R²:   {optimized_results['test_r2']:.4f}\")\n",
    "print(f\"\\nTrain MAE:  {optimized_results['train_mae']:.2f} €/m²\")\n",
    "print(f\"Test MAE:   {optimized_results['test_mae']:.2f} €/m²\")\n",
    "print(f\"\\nTrain RMSE: {optimized_results['train_rmse']:.2f} €/m²\")\n",
    "print(f\"Test RMSE:  {optimized_results['test_rmse']:.2f} €/m²\")\n",
    "\n",
    "print(\"COMPARISON TO SKLEARN OPTIMIZED\")\n",
    "print(\"sklearn results:\")\n",
    "print(\"  Train R²:  0.9493\")\n",
    "print(\"  Test R²:   0.9009\")\n",
    "print(\"  Train MAE:  246.54 €/m²\")\n",
    "print(\"  Test MAE:   375.53 €/m²\")\n",
    "print(\"  Train RMSE: 361.29 €/m²\")\n",
    "print(\"  Test RMSE:  528.34 €/m²\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feature_importance",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "show_importance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 Most Important Features:\n",
      "                    feature_names  coefficients\n",
      "6           num__days_since_start      0.079924\n",
      "0                num__code_postal      0.074822\n",
      "12       num__station_surface_std      0.067462\n",
      "17     num__station_surface_range      0.062694\n",
      "5                   num__latitude      0.055374\n",
      "15         num__station_rooms_std      0.050895\n",
      "10          num__station_tx_count      0.050651\n",
      "27  num__total_transactions_train      0.050197\n",
      "26         num__transaction_count      0.049801\n",
      "4                  num__longitude      0.049553\n"
     ]
    }
   ],
   "source": [
    "# Create feature importance dataframe\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature_names': [f'num__{f}' for f in numeric_features],\n",
    "    'coefficients': optimized_model.feature_importances_\n",
    "})\n",
    "\n",
    "feature_importance_sorted = feature_importance.sort_values('coefficients', ascending=False)\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features:\")\n",
    "print(feature_importance_sorted.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "visualize_importance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "alignmentgroup": "True",
         "hovertemplate": "Features=%{x}<br>Importance=%{y}<extra></extra>",
         "legendgroup": "",
         "marker": {
          "color": "#636efa",
          "pattern": {
           "shape": ""
          }
         },
         "name": "",
         "offsetgroup": "",
         "orientation": "v",
         "showlegend": false,
         "textposition": "auto",
         "type": "bar",
         "x": [
          "num__days_since_start",
          "num__code_postal",
          "num__station_surface_std",
          "num__station_surface_range",
          "num__latitude",
          "num__station_rooms_std",
          "num__station_tx_count",
          "num__total_transactions_train",
          "num__transaction_count",
          "num__longitude",
          "num__dist_center",
          "num__station_avg_surface",
          "num__station_avg_rooms",
          "num__market_activity_ratio",
          "num__geo_cluster",
          "num__rooms_vs_station_avg",
          "num__surface_vs_station_avg",
          "num__nearest_metro_dist_km",
          "num__station_median_surface",
          "num__surface_per_piece",
          "num__metro_count_500m",
          "num__surface_reelle_bati",
          "num__metro_count_300m",
          "num__nombre_pieces_principales",
          "num__larger_than_station_median",
          "num__station_median_rooms",
          "num__is_studio",
          "num__very_close_to_metro",
          "num__is_large"
         ],
         "xaxis": "x",
         "y": [
          0.07992381770688849,
          0.07482241864662488,
          0.06746158803181007,
          0.06269445066807962,
          0.055374018336190496,
          0.050895322703346034,
          0.05065140951479473,
          0.05019689872991638,
          0.04980097934055304,
          0.04955324723609335,
          0.044040966362192076,
          0.04102019632033251,
          0.04013587327079729,
          0.03251245911520782,
          0.030747911638461304,
          0.03019071148411798,
          0.028558549333592073,
          0.027848891193009616,
          0.027710855026517735,
          0.027275182105249193,
          0.02702238736728619,
          0.025424188769012663,
          0.0090653889750012,
          0.00797686963383532,
          0.004635556895619091,
          0.002571562972431734,
          0.0010764164872372182,
          0.00042605142924647527,
          0.0003858307065554066
         ],
         "yaxis": "y"
        }
       ],
       "layout": {
        "barmode": "relative",
        "legend": {
         "tracegroupgap": 0
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "Random Forest Feature Importance - NumPy Implementation"
        },
        "xaxis": {
         "anchor": "y",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Features"
         }
        },
        "yaxis": {
         "anchor": "x",
         "domain": [
          0,
          1
         ],
         "title": {
          "text": "Importance"
         }
        }
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize feature importance\n",
    "fig1 = px.bar(\n",
    "    feature_importance_sorted,\n",
    "    x=\"feature_names\",\n",
    "    y=\"coefficients\",\n",
    "    title=\"Random Forest Feature Importance - NumPy Implementation\",\n",
    "    labels={\n",
    "        'coefficients': 'Importance',\n",
    "        'feature_names': 'Features'\n",
    "    }\n",
    ")\n",
    "\n",
    "fig1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "save_model",
   "metadata": {},
   "source": [
    "## Save Model and Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "save_artifacts",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model and preprocessor saved successfully!\n",
      "  - model_numpy_optimized.pkl\n",
      "  - preprocessor_numpy.pkl\n"
     ]
    }
   ],
   "source": [
    "# Create preprocessor object\n",
    "preprocessor = {\n",
    "    'imputer': imputer,\n",
    "    'scaler': scaler,\n",
    "    'feature_names': numeric_features\n",
    "}\n",
    "\n",
    "# Save optimized model\n",
    "with open(\"../src/model_numpy.pkl\", \"wb\") as f:\n",
    "    pickle.dump(optimized_model, f)\n",
    "\n",
    "# Save preprocessor\n",
    "with open(\"../src/preprocessor_numpy.pkl\", \"wb\") as f:\n",
    "    pickle.dump(preprocessor, f)\n",
    "\n",
    "print(\"\\nModel saved\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook reproduces the sklearn pipeline EXACTLY using only NumPy:\n",
    "\n",
    "**Key Matches:**\n",
    "- Same target: `price_per_sqrtm`\n",
    "- Same aggregation: by (code_postal, year, month)\n",
    "- Same preprocessing: median imputation + standardization\n",
    "- Same model parameters: Random Forest with absolute_error criterion\n",
    "- Baseline: 200 trees, depth 30, min_samples_split 15\n",
    "- Optimized: 250 trees, depth 25, min_samples_split 25\n",
    "\n",
    "**Custom NumPy Implementations:**\n",
    "- K-Means clustering\n",
    "- Ball Tree (haversine distance)\n",
    "- Standard Scaler\n",
    "- Simple Imputer (median strategy)\n",
    "- Decision Tree Regressor (with MAE criterion)\n",
    "- Random Forest Regressor\n",
    "- Evaluation metrics (MAE, RMSE, R²)\n",
    "\n",
    "Now you can directly compare NumPy vs sklearn performance on the SAME problem!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
